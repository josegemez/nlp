{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install torchtext --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install torchdata --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" \n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cuda')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import IMDB \n",
    "train_dataset = IMDB(split='train') \n",
    "test_dataset = IMDB(split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12500"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split \n",
    "torch.manual_seed(1) \n",
    "train_dataset, valid_dataset = random_split(list(train_dataset), [12000, 500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "from collections import Counter, OrderedDict \n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text) \n",
    "    emoticons = re.findall( '(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower() )  \n",
    "    text = re.sub('[\\W]+', ' ', text.lower())+' '.join(emoticons).replace('-', '')\n",
    "    tokenized = text.split()\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab-size: 54011\n"
     ]
    }
   ],
   "source": [
    "token_counts = Counter() \n",
    "for label, line in train_dataset: \n",
    "    tokens = tokenizer(line) \n",
    "    token_counts.update(tokens) \n",
    "print('Vocab-size:', len(token_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pepe = tokenizer(next(iter(train_dataset))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3: encoding each unique token into integers \n",
    "from torchtext.vocab import vocab \n",
    "sorted_by_freq_tuples = sorted(  token_counts.items(), key=lambda x: x[1], reverse=True  ) \n",
    "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "vocab = vocab(ordered_dict)\n",
    "vocab.insert_token(\"<pad>\", 0)\n",
    "vocab.insert_token(\"<unk>\", 1)\n",
    "vocab.set_default_index(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 7, 40, 431]\n"
     ]
    }
   ],
   "source": [
    "print([vocab[token] for token in ['this', 'is', 'an', 'example']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
    "label_pipeline = lambda x: 1. if x == 'pos' else 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch): \n",
    "    label_list, text_list, lengths = [], [], [] \n",
    "    for _label, _text in batch: \n",
    "        label_list.append(label_pipeline(_label)) \n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64) \n",
    "        text_list.append(processed_text) \n",
    "        lengths.append(processed_text.size(0)) \n",
    "    label_list = torch.tensor(label_list) \n",
    "    lengths = torch.tensor(lengths) \n",
    "    padded_text_list = nn.utils.rnn.pad_sequence( text_list, batch_first=True) \n",
    "    return padded_text_list, label_list, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader \n",
    "dataloader = DataLoader(train_dataset, batch_size=4, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  165,     9,   793,     9,   166,     6,  3232,   125,    12,    11,\n",
       "           15,    10,     4,     5,   405,     7,    67,    90,    87,     4,\n",
       "           90,   899,    38,    33,   542,   702, 33216,    41,   974,  1095,\n",
       "           32,  6250,     8,     7,  1038,   527,     4,   642,    10,     2,\n",
       "          211,  1674,    19,     3,   175,     5,  1375,     8,     7,     3,\n",
       "           67,    54,    15,    16,    12,     9,   195,     8,     2,   119,\n",
       "          399,   203,   247,    75,  1925,   117,    62,    18,    46,    22,\n",
       "           51,   311,     3,  2674,   244,  2675,     9,   166,   234,     6,\n",
       "          194,    45,     2,  1363,  2732,  1166,     5,    11,    21,     4,\n",
       "           83,  2014,     8,     7,     6,  1892,    80,     3,   175,     5,\n",
       "         1375,    19,    43,     8,     7,   961,     9,  4284,    12,    11,\n",
       "           21,   876,    10,     2,   737,  1472,   209,   112,    12,     2,\n",
       "          278,   107,   866,  4285, 22091,     7,     3,   500,  8900,     5,\n",
       "         1375,   247,     8,  2769,    11,   107,    86,     3,   161,  1438,\n",
       "         1056,     4,  3056,     2, 22092,   551,     6,  1719,    28,    14,\n",
       "           10,   193,   161,     4,    46,    30,    26,   333,     7,     2,\n",
       "          961,     9,    25,    48,   352,    18,     2, 15807,     5,   193,\n",
       "            4,  1375,    17,     6,    73,    34,    10,   143,     3, 11226,\n",
       "          598,     7,    10,     3,   567, 12623,    36,    51,    27,    48,\n",
       "          746,    12,  2676,   410,    18,    98,   815,   252,  3846,     7,\n",
       "        11867,    18,   534,  3949,     4,  2035,   386,    52,   200,  3057,\n",
       "            6,  2049,     2,  2035,     5,    80,    10,   136,  3846,    23,\n",
       "           21,    17,   386,   102,    52,   104,   831,  1976,   205,   212,\n",
       "            2,    57,   387,    96,     6,  2049,     2,  2035,    12,   174,\n",
       "            6,   543,  2410,    17,     2,  1268,     7,     6,   285,   715,\n",
       "            8,    16,   553,    39,     6,    31,   176,    25,     8,  2733,\n",
       "           10,    40,  1292,   766,    17,   136,  6776,     5,  2732,   103,\n",
       "           10,   161,  1438,  3351,    51,    57,    73,    34,    77,    31,\n",
       "            2,   128,     5,     2,   298,    30,   510,   312,    44,  5532,\n",
       "            2,  2036,    16,     2,   551,     6,  1041,    16,   361,     4,\n",
       "           58,    44,  1047,    22,    42,  1498,     6,  1498,     4,  2983,\n",
       "           22,   113,     2,   366,  1281,    69,    71,   661,    22,     8,\n",
       "           14,    53,     2, 14556,   431,     5,    11,    51,    27,   268,\n",
       "           38,   242,  1731,     6,   105,     2,   766,    23,     2,   327,\n",
       "          100,  1018,   144,     2,    15,  5295,     2,    98,  1566,  1498,\n",
       "            5,     2,    21,     7,     2,  1042,     5,     2,   540,     5,\n",
       "         4079,  8901, 33217,    13,   349,   344,  1049,    28,   466,    59,\n",
       "           28,  2355,    49,  9284,    10,     2,    21,    71,    69,    12,\n",
       "           49,  8230,    25,    76,  4011,     4, 26160,    10,  6968,  2544,\n",
       "           16,  2563,     4,  4686,  7433,    11,     7,     2,   152,   205,\n",
       "          239,    22,   395,    12,  8901,     7,    19,     2, 33218,   125,\n",
       "        33219,    53,    17,   140,     2,   766,    71,   900,    12,    11,\n",
       "            7,    10,   193,   124,    58,    75,     3,  6103,    66,     2,\n",
       "         1018,  6251,     5,  5071,    31,     2,    62,   361,   908,   209,\n",
       "            7,    24,   387,    34,    46,    30,    37,    82,    56,   136,\n",
       "          192,   109,    79,   386,    52,     3,   282,   696,     5,   323,\n",
       "         1386,    12,    13,   109,    30,    70,    20,  1725,   209,     6,\n",
       "          216,     6,     2,   211,  1108,  3789,   158,    12,    60,    14,\n",
       "         1683,     4, 12624,     7,    10,   405,   177,    16,   209,     6,\n",
       "         4687,    12,  8901,    14,    24,     2,  1690,   181,    28,  1120,\n",
       "            6,     2,  1965,    71,    70,    20,   351,    11,  2287,  4140,\n",
       "         3058,  2070,  3059,     6, 15808,    12,     4,    11,     7,    37,\n",
       "            2,  4688,     5,     2,  9670,    46,     7,   102,    24,  1357,\n",
       "            6,     2,  7660,   551,     7,    12,   470,   107, 22091,     7,\n",
       "          361,  2732,    36,    14,    48,   261,  5193,   874,   642,   479,\n",
       "         8901,    13,  2803,    19,   143, 22091,     7,   624,  1190,    57,\n",
       "           16,     2,  1530,     5,  3276,   209,   628, 14557,   395,     2,\n",
       "         2369,   252,     2,   872,   301,     6,   132,    71,   426,    20,\n",
       "           27,   988,    10,     3,    21,    44,  2676,   592,    36,     7,\n",
       "            3,   436,   398,    10,     8,  4439,    50,     2,  7921,   107,\n",
       "            7,  1120,    38,    40,   336,   316,   279,     2,   352,    18,\n",
       "           11,     7,    12,     2,    15,     7,    48,  1178,    40,  5967,\n",
       "            5,  8901,     4,    32,  9285,    17,   234,    40,  6969,   785,\n",
       "           44,     3,   436, 19350,   616,     6,  2676,    16,    33,     2,\n",
       "          297,   975,   205,   505,   319,  2124,     4,    81,   980,   361,\n",
       "           50,    31,     2,  2050,     5,   308,    88,  1745,   619,   901,\n",
       "            9,    25,     6,   125,     8,     7,   728, 12623,     6,  1396,\n",
       "           12,     2,   161,  2591,   363,     5,  2203,  8901,    14,     2,\n",
       "          828,     5,     3, 26161,    18,     3,  2732,  5193,   874,     8,\n",
       "          218,   174,     6,   510,     3,  5194,   632,    16,     2,  1530,\n",
       "            5,   742,     9,    61,   656,   386,    55,   513, 11868,   209,\n",
       "          184,    16,   323,   212,     9,    89,  1589,   104, 22093,     6,\n",
       "           73,    11,    18,     2,  5533,  4080,     3,   521,   337,  2288,\n",
       "         2447,     4, 26162, 33220,    13, 11869,  1675,  1293,     5,     2,\n",
       "          148,     5,   379,  9671,    66,  4603,     2,  7179,   568,     5,\n",
       "            2,   145,     6,  1190,    57,     2,  3677,   584,    12,   951,\n",
       "           37,    51,    20,    78,   177,     5,     2,   621,   961,     7,\n",
       "           12,   386,    52,    76,  3439,   209,   136, 19351,    16,   202,\n",
       "            4,   889,    41,    26,  5072,     4,  7434,    92,  1966,    39,\n",
       "        15809,     6,  3521,    16,   545,   809,   136,   817, 19352,  4141,\n",
       "          329,     4, 22094,   158,     9,   122,   494,    24,  7180,  8901,\n",
       "            7,   110,   610,    86,     3,   897,    28,   230,    20,    64,\n",
       "          194,     7,    18,     2,   961,   110,    34,  1566,   177,     6,\n",
       "         1987,   209,     5,     2,  5813,     5,     2,   145,   109,   224,\n",
       "          386,    81,   137,     6,   143,  2732, 10137,   109,   224,   386,\n",
       "         1813,     6,  1445,   209,    38,  2204,   323,   921,   144,   136,\n",
       "          121,   109,    51,    30,    24,  1725,   209,     6,   101,    16,\n",
       "         3789,    51,    71,    24,    37,   269,     2,  1292,  7922,    23,\n",
       "          283,     4,   251,     2,   292,  1041,    16,   545,     9,  3728,\n",
       "           33,    41,  1813,     6,   105,   386,    13, 14558,   387,    94,\n",
       "            6,    73,   553,     2,  1606,     5,   296,  5407,    16,   553,\n",
       "            4,     6,   114,  1719,    12,    46,     7,    23,   283,     7,\n",
       "           50,   511,     6,     2,   961])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloader))[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dl = DataLoader(train_dataset, batch_size=batch_size,  shuffle=True, collate_fn=collate_batch, pin_memory=False) \n",
    "valid_dl = DataLoader(valid_dataset, batch_size=batch_size,  shuffle=False, collate_fn=collate_batch, pin_memory=False)\n",
    "test_dl = DataLoader(test_dataset, batch_size=batch_size,  shuffle=False, collate_fn=collate_batch, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(  num_embeddings=10,  embedding_dim=3,  padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module): \n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size,  fc_hidden_size): \n",
    "        super().__init__() \n",
    "        self.embedding = nn.Embedding(vocab_size,  embed_dim,  padding_idx=0) \n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size,  batch_first=True) \n",
    "        self.fc1 = nn.Linear(rnn_hidden_size, fc_hidden_size) \n",
    "        self.relu = nn.ReLU()  \n",
    "        self.fc2 = nn.Linear(fc_hidden_size, 1) \n",
    "        self.sigmoid = nn.Sigmoid()  \n",
    "    def forward(self, text, lengths): \n",
    "        out = self.embedding(text) \n",
    "        out = nn.utils.rnn.pack_padded_sequence(  out, lengths.cpu().numpy(), enforce_sorted=False, batch_first=True  ) \n",
    "        out, (hidden, cell) = self.rnn(out) \n",
    "        out = hidden[-1, :, :] \n",
    "        out = self.fc1(out) \n",
    "        out = self.relu(out) \n",
    "        out = self.fc2(out) \n",
    "        out = self.sigmoid(out) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab) \n",
    "embed_dim = 20 \n",
    "rnn_hidden_size = 64 \n",
    "fc_hidden_size = 64 \n",
    "torch.manual_seed(1) \n",
    "model = RNN(vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader): \n",
    "    model.train() \n",
    "    total_acc, total_loss = 0, 0 \n",
    "    for text_batch, label_batch, lengths in dataloader: \n",
    "        optimizer.zero_grad() \n",
    "        pred = model(text_batch, lengths)[:, 0] \n",
    "        loss = loss_fn(pred, label_batch) \n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "        total_acc += (  (pred >= 0.5).float() == label_batch  ).float().sum().item() \n",
    "        total_loss += loss.item()*label_batch.size(0) \n",
    "    return total_acc/len(dataloader.dataset),  total_loss/len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader): \n",
    "    model.eval() \n",
    "    total_acc, total_loss = 0, 0\n",
    "    with torch.no_grad(): \n",
    "        for text_batch, label_batch, lengths in dataloader: \n",
    "            pred = model(text_batch, lengths)[:, 0] \n",
    "            loss = loss_fn(pred, label_batch) \n",
    "            total_acc += (  (pred>=0.5).float() == label_batch  ).float().sum().item() \n",
    "            total_loss += loss.item()*label_batch.size(0) \n",
    "    return total_acc/len(dataloader.dataset),  total_loss/len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 accuracy:1.0000 val_accuracy: 1.0000\n",
      "Epoch 1 accuracy:1.0000 val_accuracy: 1.0000\n",
      "Epoch 2 accuracy:1.0000 val_accuracy: 1.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\github\\nlp\\nlp.ipynb Cell 26\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/github/nlp/nlp.ipynb#X33sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m torch\u001b[39m.\u001b[39mmanual_seed(\u001b[39m1\u001b[39m) \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/github/nlp/nlp.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs): \n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/github/nlp/nlp.ipynb#X33sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     acc_train, loss_train \u001b[39m=\u001b[39m train(train_dl) \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/github/nlp/nlp.ipynb#X33sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     acc_valid, loss_valid \u001b[39m=\u001b[39m evaluate(valid_dl) \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/github/nlp/nlp.ipynb#X33sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m accuracy:\u001b[39m\u001b[39m{\u001b[39;00macc_train\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m  \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m val_accuracy: \u001b[39m\u001b[39m{\u001b[39;00macc_valid\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32md:\\github\\nlp\\nlp.ipynb Cell 26\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(dataloader)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/github/nlp/nlp.ipynb#X33sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m pred \u001b[39m=\u001b[39m model(text_batch, lengths)[:, \u001b[39m0\u001b[39m] \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/github/nlp/nlp.ipynb#X33sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(pred, label_batch) \n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/github/nlp/nlp.ipynb#X33sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward() \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/github/nlp/nlp.ipynb#X33sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep() \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/github/nlp/nlp.ipynb#X33sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m total_acc \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (  (pred \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m)\u001b[39m.\u001b[39mfloat() \u001b[39m==\u001b[39m label_batch  )\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mitem() \n",
      "File \u001b[1;32mc:\\Users\\jose\\anaconda3\\envs\\torch_38\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    490\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\jose\\anaconda3\\envs\\torch_38\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10 \n",
    "torch.manual_seed(1) \n",
    "for epoch in range(num_epochs): \n",
    "    acc_train, loss_train = train(train_dl) \n",
    "    acc_valid, loss_valid = evaluate(valid_dl) \n",
    "    print(f'Epoch {epoch} accuracy:{acc_train:.4f}'  f' val_accuracy: {acc_valid:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f0748064ac020567765afaa2e33e46dd13e2611d6c7241060e2e86181936ea6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
